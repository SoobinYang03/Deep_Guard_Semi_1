import requests
from bs4 import BeautifulSoup
import re
import asyncio
from telethon import TelegramClient
from urllib.parse import quote
import uuid
from datetime import datetime
import os
from dotenv import load_dotenv
import aiohttp
import google.generativeai as genai

load_dotenv(dotenv_path='auth.env')

# [ì„¤ì • ë¶€ë¶„]
tor_port = 9050
tor_proxy = f"socks5h://127.0.0.1:{tor_port}"
proxies = {'http': tor_proxy, 'https': tor_proxy}
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Gecko/20100101 Firefox/91.0'}

tg_api_id = int(os.getenv('telegram_api_id'))
tg_api_hash = os.getenv('telegram_api_hash')
tg_session = os.getenv('telegram_session')

google_api_key = os.getenv('google_api_key')
if google_api_key:
    genai.configure(api_key=google_api_key)
else:
    print("êµ¬ê¸€apií‚¤ ë¯¸ì„¤ì •. AI ê¸°ëŠ¥ ì œí•œë¨.")

target_channels = [
    'cveNotify', 'jacuzzidf', 'fredenscombos', 'hannibalmaaleaks', 'lunarisS3C',
    'milkdude', 'marketo_leaks', 'leaked_databases', 'jokersworlds', 'DarkfeedNews',
    'D1rkSec', 'CrazyHuntersTeam', 'Combolistfresh', 'canyoupwnme', 'baseleeak',
    'APTANALYSIS', 'cbanke'
]

surface_header = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

surface_mirrors = {
    "pastebin": "https://pastebin.com/search/?q=",
    "Darkforums": "https://darkforums.st/search/?q=",
    "Ahmia": "https://ahmia.fi/search/?q=",
    "GitHub": "https://github.com/search?type=code&q="
}

darkweb_target = {
    "Ahmia": "http://juhanurmihxlp77nkq76byazcldy2hlmovfu2epvl5ankdibsot4csyd.onion/search/?q=",
    "Darkforums": "http://forums56xf3ix34sooaio4x5n275h4i7ktliy4yphhxohuemjpqovrad.onion/search.php?keywords=",
    "Torch": "http://c6txtcqza5pfilkdd65qpafpou27hfbcogb4geufxec35a4iyaxywfqd.onion/search?q="
}

threat_keywords = [
    "combo", "password", "stealer", "leak", "database", "auth", "dump",
    "fullz", "login", "hack", "email:pass", "id:pass"
]
asset_keywords = ["admin", "root", "confidential", "secret", "backup", "internal", "intranet", "vpn"]
project_keywords = ["source code", "blueprint", "schema", "design", "confidential", "internal only", "leaked", "dump",
                    "api key"]
ransomware_archive = "https://raw.githubusercontent.com/joshhighet/ransomwatch/main/posts.json"

# [NEW] ê°€ì§œ ë°ì´í„° í•„í„°ë§ì„ ìœ„í•œ ì œì™¸ í‚¤ì›Œë“œ (ë¸”ë™ë¦¬ìŠ¤íŠ¸)
SKIP_KEYWORDS = [
    "login", "signin", "sign up", "register", "terms", "privacy", "policy",
    "about", "contact", "pricing", "blog", "news", "help", "support", "status",
    "docs", "documentation", "api", "jobs", "careers", "press", "legal",
    "cookie", "sitemap", "advertisement", "subscribe", "donate"
]


def parse_company_info(email):
    try:
        domain = email.split('@')[1]
        company = domain.split('.')[0]
        return domain, company
    except:
        return None, None


def get_competitors_by_ai(company_name):
    if not google_api_key: return ["competitor_a"]
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        prompt = (f"List top 3 major competitors of '{company_name}'. Output only names separated by comma.")
        response = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(temperature=0.0))
        return [x.strip() for x in response.text.split(',')]
    except:
        return ["competitor_a"]


def add_tag(results_list, tag_name):
    for item in results_list: item['keyword_type'] = tag_name
    return results_list


# [ìˆ˜ì •] keyword_type íŒŒë¼ë¯¸í„° ì¶”ê°€ ë° ê¸°ë³¸ê°’ ì„¤ì •
def format_database(source, text, url, leak_date, target_email=None, found_keyword=None, keyword_type="credential"):
    return {
        "id": str(uuid.uuid4()),
        "keyword_type": keyword_type,  # ì—¬ê¸°ì„œ íƒ€ì…ì„ ê²°ì •í•¨
        "source_id": source,
        "original_link": url,
        "raw_text": text,
        "leak_date": str(leak_date),
        "target_email": target_email,
        "found_keyword": found_keyword
    }


def spamfilter(text):
    if not text or len(text) < 20: return True
    if any(k in text.lower() for k in ["join my channel", "promo code", "casino"]): return True
    return False


def get_matching_keyword(text, keyword_list):
    if not text: return None
    text_lower = text.lower()
    for k in keyword_list:
        if k in text_lower: return k
    return None


def find_any_threat_keyword(text):
    return get_matching_keyword(text, threat_keywords + asset_keywords + project_keywords)


# -------------------------------------------------------------------------
# ì„œí”¼ìŠ¤ ì›¹ ê²€ìƒ‰
# -------------------------------------------------------------------------
async def fetch_surface_url(session, name, base_url, keyword):
    search_url = f"{base_url}{quote(keyword)}"
    results = []
    try:
        async with session.get(search_url, headers=surface_header, timeout=50) as response:
            if response.status == 200:
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                links = []

                for a in soup.find_all('a', href=True):
                    href = a['href']
                    text = a.get_text().strip().lower()

                    if href.startswith('/'):
                        if "github" in base_url:
                            href = f"https://github.com{href}"
                        elif "pastebin" in base_url:
                            href = f"https://pastebin.com{href}"

                    if not href.startswith('http'): continue
                    if any(bad_word in href.lower() or bad_word in text for bad_word in SKIP_KEYWORDS): continue
                    if (keyword.lower() not in href.lower()) and (keyword.lower() not in text): continue

                    links.append(href)

                for link in list(set(links))[:3]:
                    if "login" in link or "signup" in link: continue
                    detected_word = find_any_threat_keyword(link)

                    data = format_database(
                        source=f"surface({name})",
                        text=f"Detected Link: {link}",
                        url=link,
                        leak_date=datetime.now(),
                        target_email=keyword,
                        found_keyword=detected_word if detected_word else keyword,
                        keyword_type="credential"  # ê¸°ë³¸ê°’ ì„¤ì •
                    )
                    results.append(data)
    except:
        pass
    return results


async def search_surface_mirroring(keyword):
    print(f"\në¯¸ëŸ¬ë§ ë° ì„œí”¼ìŠ¤ ì›¹ ê²€ìƒ‰ ì‹œì‘: {keyword}")
    all_results = []
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_surface_url(session, name, url, keyword) for name, url in surface_mirrors.items()]
        results_list = await asyncio.gather(*tasks)
        for res in results_list: all_results.extend(res)
    return all_results


# -------------------------------------------------------------------------
# ë‹¤í¬ì›¹ ê²€ìƒ‰
# -------------------------------------------------------------------------
def search_darkweb(keyword):
    print(f"\në‹¤í¬ì›¹ ê²€ìƒ‰ ì‹œì‘: {keyword}")
    results = []
    for name, base_url in darkweb_target.items():
        search_url = f"{base_url}{quote(keyword)}"
        try:
            res = requests.get(search_url, headers=headers, proxies=proxies, timeout=60)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text, 'html.parser')
                links = []
                for a in soup.find_all('a', href=True):
                    href = a['href']
                    if '.onion' in href and 'http' in href: links.append(href)

                for link in list(set(links))[:3]:
                    try:
                        page_res = requests.get(link, headers=headers, proxies=proxies, timeout=15)
                        if keyword in page_res.text:
                            page_soup = BeautifulSoup(page_res.text, 'html.parser')
                            for s in page_soup(['script', 'style']): s.extract()
                            clean_text = ' '.join(page_soup.stripped_strings)

                            detected_word = find_any_threat_keyword(clean_text)
                            start_idx = clean_text.find(keyword)
                            start = max(0, start_idx - 50)
                            end = min(len(clean_text), start_idx + 150)
                            snippet = clean_text[start:end]

                            data = format_database(
                                source=f"darkweb ({name})",
                                text=snippet,
                                url=link,
                                leak_date=datetime.now(),
                                target_email=keyword,
                                found_keyword=detected_word if detected_word else "Darkweb Context",
                                keyword_type="credential"  # ê¸°ë³¸ê°’ ì„¤ì •
                            )
                            results.append(data)
                    except:
                        continue
        except:
            pass
    return results


# -------------------------------------------------------------------------
# í…”ë ˆê·¸ë¨ ê²€ìƒ‰
# -------------------------------------------------------------------------
async def search_telegram(client, keyword, keyword_type):
    print(f"\ní…”ë ˆê·¸ë¨ ê²€ìƒ‰: {keyword}({keyword_type})")
    results = []
    if keyword_type == "credential":
        check_list = threat_keywords
    elif keyword_type == "asset":
        check_list = asset_keywords
    elif keyword_type == "project":
        check_list = project_keywords
    else:
        check_list = []

    for channel in target_channels:
        try:
            async for message in client.iter_messages(channel, search=keyword, limit=200):
                if spamfilter(message.text): continue
                matched_word = get_matching_keyword(message.text, check_list)
                if not matched_word: continue

                data = format_database(
                    source=f"telegram({channel})",
                    text=message.text,
                    url=f"https://t.me/{channel}/{message.id}" if 'http' not in channel else f"{channel}/{message.id}",
                    leak_date=message.date,
                    target_email=keyword,
                    found_keyword=matched_word,
                    keyword_type=keyword_type  # ê²€ìƒ‰ ìš”ì²­ë°›ì€ íƒ€ì… ê·¸ëŒ€ë¡œ ì „ë‹¬
                )
                results.append(data)
        except:
            continue
    return results


# -------------------------------------------------------------------------
# ëœì„¬ì›¨ì–´ ê²€ìƒ‰
# -------------------------------------------------------------------------
def search_ransomware(target_companies):
    results = []
    try:
        res = requests.get(ransomware_archive, timeout=10)
        if res.status_code == 200:
            all_attacks = res.json()
            for attack in all_attacks:
                victim_name = str(attack.get('post_title', ''))
                for company in target_companies:
                    if re.search(r'\b' + re.escape(company) + r'\b', victim_name, re.IGNORECASE):
                        group = attack.get('group_name', 'Unknown')
                        date = attack.get('discovered', 'Unknown')
                        data = format_database(
                            source=f"ransomware({group})",
                            text=f"ê³µê²© ëŒ€ìƒ: {victim_name}",
                            url=f"https://ransomlook.io/group/{group}",
                            leak_date=date,
                            target_email=None,
                            found_keyword=f"Ransomware: {group}",
                            keyword_type="asset"  # íšŒì‚¬ ìœ ì¶œì€ ìì‚° ìœ„í˜‘ìœ¼ë¡œ ë¶„ë¥˜
                        )
                        results.append(data)
    except:
        pass
    return results


# -------------------------------------------------------------------------
# ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬ (ë°ëª¨ ëª¨ë“œ + ì¤‘ë³µ ì œê±°ëŠ” Main.pyì—ì„œ ì²˜ë¦¬í•˜ì§€ë§Œ ì—¬ê¸°ì„œë„ íƒœê·¸ëŠ” í™•ì‹¤íˆ)
# -------------------------------------------------------------------------
async def main_controller(email, keyword):
    all_findings = []

    # [ë°ëª¨ ëª¨ë“œ] íŠ¹ì • ê³„ì • ê²€ìƒ‰ ì‹œ ê°•ì œ ê²°ê³¼ ë°˜í™˜ (ì‹œì—°ìš©)
    if email == "demo@deepguard.com":
        print("ğŸš¨ [Demo Mode] ë°ëª¨ ê³„ì • ê°ì§€ - ê°€ì§œ ìœ ì¶œ ë°ì´í„° ìƒì„±")
        all_findings.append(format_database(
            source="darkweb (RaidForums)",
            text="[COMBO] Email:pass list dump... found_keyword: password matched.",
            url="http://hss33ml644n4.onion/leaks/database/123",
            leak_date=datetime.now(),
            target_email=email,
            found_keyword="password",
            keyword_type="credential"
        ))

        # 2. ê¸°ì¡´ ê°€ì§œ ë°ì´í„° (Project)
        all_findings.append(format_database(
            source="surface(Pastebin)",
            text="Project Titan API Keys exposed... found_keyword: api key",
            url="https://pastebin.com/raw/k123kk",
            leak_date=datetime.now(),
            target_email=email,
            found_keyword="api key",
            keyword_type="project"
        ))

        # ---------------------------------------------------------
        # [ì¶”ê°€ë¨] 3. ê²½ìŸì‚¬/ìì‚¬ ëœì„¬ì›¨ì–´ í”¼í•´ ë‚´ì—­ (ê°•ì œ ì£¼ì…)
        # ---------------------------------------------------------
        # ì‹œì—° ì‹œë‚˜ë¦¬ì˜¤: "DeepGuardëŠ” AIë¡œ ê²½ìŸì‚¬ë¥¼ ì‹ë³„í•˜ì—¬, ê·¸ë“¤ì˜ í”¼í•´ ì‚¬ë¡€ë„ í•¨ê»˜ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤."

        # ê²½ìŸì‚¬ 1: LG Electronics (LockBit)
        all_findings.append(format_database(
            source="ransomware(LockBit 3.0)",
            text="ê³µê²© ëŒ€ìƒ: LG Electronics Service Data / Status: Published",
            url="https://ransomlook.io/group/lockbit3",
            leak_date="2024-01-15",
            target_email=None,
            found_keyword="Ransomware: LockBit",
            keyword_type="company"  # í”„ë¡ íŠ¸ì—ì„œ ìƒ‰ìƒì´ ë‹¤ë¥´ê²Œ ë³´ì¼ ìˆ˜ ìˆìŒ (í˜¹ì€ assetìœ¼ë¡œ ë³€ê²½)
        ))

        # ê²½ìŸì‚¬ 2: SK Hynix (BlackCat)
        all_findings.append(format_database(
            source="ransomware(ALPHV/BlackCat)",
            text="ê³µê²© ëŒ€ìƒ: SK Hynix Internal Schematics / Status: Leaked",
            url="https://ransomlook.io/group/alphv",
            leak_date="2023-11-20",
            target_email=None,
            found_keyword="Ransomware: ALPHV",
            keyword_type="company"
        ))
        return all_findings

    # [ì‹¤ì œ í¬ë¡¤ë§]
    domain, company = parse_company_info(email)
    if not domain: company = "Unknown"

    print(f"DeepGuard ì§„ë‹¨ ì‹œì‘: {company} ({domain})")
    competitors = get_competitors_by_ai(company) if company != "Unknown" else []

    async with TelegramClient(tg_session, tg_api_id, tg_api_hash) as client:
        # 1. Credential
        res = await search_telegram(client, email, "credential")
        all_findings.extend(res)  # add_tag í˜¸ì¶œ ë¶ˆí•„ìš” (í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•¨)

        res = await search_surface_mirroring(email)
        all_findings.extend(res)

        res = search_darkweb(email)
        all_findings.extend(res)

        # 2. Asset
        if domain:
            res = await search_telegram(client, domain, "asset")
            all_findings.extend(res)

        # 3. Project
        if keyword:
            res = await search_telegram(client, keyword, "project")
            for item in res:
                if not item['found_keyword']: item['found_keyword'] = "Project Leak"
            all_findings.extend(res)

            res = await search_surface_mirroring(keyword)
            # í”„ë¡œì íŠ¸ ê²€ìƒ‰ ê²°ê³¼ëŠ” íƒ€ì…ì„ projectë¡œ ê°•ì œ ë³€í™˜
            for item in res: item['keyword_type'] = "project"
            all_findings.extend(res)

        # 4. Company (Ransomware)
        if company != "Unknown":
            res = search_ransomware([company] + competitors)
            all_findings.extend(res)

    return all_findings