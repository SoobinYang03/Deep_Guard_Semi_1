import hashlib
import uuid
import re
import requests
import urllib3
import os
import tempfile
import json
import pandas as pd
from datetime import datetime
from pymongo import MongoClient
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from elasticsearch import Elasticsearch, helpers
from app.notifications.email import send_leak_alert  # ì´ë©”ì¼ ì•Œë¦¼
from typing import Optional
from bson import ObjectId
import deepguard_analyzer as dga


# ---------------------------------------------------------
# [NEW] í¬ë¡¤ëŸ¬ ëª¨ë“ˆ ì„í¬íŠ¸ (ì¶”ê°€ëœ ë¶€ë¶„)
# ---------------------------------------------------------
try:
    from deepguard_crawl_b2b import main_controller, project_keywords
except ImportError:
    print("âš ï¸ ê²½ê³ : 'deepguard_crawl_b2b' ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("âš ï¸ ì„ì‹œ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")


    async def main_controller(target):
        return {
            "id": str(uuid.uuid4()),
            "target_email": target,
            "status": "Test Mode (Crawler Not Found)",
            "leaked_date": str(datetime.now()),
            "source": "Test Source"
        }

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000"
    ],  # React ê°œë°œ ì„œë²„
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

es_url = os.getenv("elasticsearch_url", "http://elasticsearch:9200")
es = Elasticsearch(
    es_url,
    verify_certs=False
)

# MongoDB ì—°ê²°
mongo_url = os.getenv("mongodb_url", "mongodb://admin:admin123@mongodb:27017/")
mongo_client = MongoClient(mongo_url)
db = mongo_client["leak_database"]
# í¬ë¡¤ëŸ¬ ì „ìš© ì»¬ë ‰ì…˜ ì¶”ê°€ (í˜¸í™˜ì„± í™•ë³´)
mongo_collection = db["leaked_data"]


# ObjectIdë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def serialize_doc(doc):
    if doc and '_id' in doc:
        doc['_id'] = str(doc['_id'])
    if doc and 'source_id' in doc:
        doc['source_id'] = str(doc['source_id'])
    if doc and 'leak_id' in doc:
        doc['leak_id'] = str(doc['leak_id'])
    return doc


def load_file_to_dataframe(file_path, content_type):
    """íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë³€í™˜"""
    _, ext = os.path.splitext(file_path)
    ext = ext.lower()

    if ext == '.csv' or content_type == 'text/csv':
        df = pd.read_csv(file_path)
    elif ext == '.tsv' or content_type == 'text/tab-separated-values':
        df = pd.read_csv(file_path, sep='\t')
    elif ext == '.json' or content_type == 'application/json':
        df = pd.read_json(file_path)
    elif ext == '.ndjson':
        df = pd.read_json(file_path, lines=True)
    else:
        # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜ (ì—ëŸ¬ ë°©ì§€)
        return pd.DataFrame()

    return df.fillna('')


def doc_generator(df, index_name):
    """Bulk APIë¥¼ ìœ„í•œ Generator"""
    for index, row in df.iterrows():
        yield {
            "_index": index_name,
            "_source": row.to_dict()
        }


# ---------------------------------------------------------
# ê¸°ì¡´ API ì—”ë“œí¬ì¸íŠ¸ë“¤ (100% ìœ ì§€)
# ---------------------------------------------------------

@app.get("/api/sources")
async def get_sources():
    """ëª¨ë“  ì¶œì²˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        sources = list(db.sources.find())
        return {"total": len(sources), "sources": [serialize_doc(s) for s in sources]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/upload")
async def upload_leak_file(
        file: UploadFile = File(...),
        source_id: Optional[str] = Form(None),
        source_name: Optional[str] = Form(None),
        source_type: Optional[str] = Form(None),
        source_description: Optional[str] = Form(None),
        original_link: Optional[str] = Form(None),
        leak_description: str = Form(...),
        leak_date: str = Form(...),
        severity: str = Form(...),
        file_name: str = Form(...)
):
    """ìœ ì¶œ íŒŒì¼ ì—…ë¡œë“œ ë° Elasticsearch ì¸ë±ì‹±"""
    try:
        # 1. Source ì²˜ë¦¬
        if source_id:
            source = db.sources.find_one({"_id": ObjectId(source_id)})
            if not source:
                raise HTTPException(status_code=404, detail="Source not found")
            final_source_id = source['_id']
        else:
            if not source_name or not source_type:
                raise HTTPException(status_code=400, detail="source_name and source_type are required for new source")
            source_doc = {
                "name": source_name,
                "type": source_type,
                "description": source_description,
                "status": "active",
                "created_at": datetime.now()
            }
            result = db.sources.insert_one(source_doc)
            final_source_id = result.inserted_id

        # 2. Leak ìƒì„±
        leak_doc = {
            "source_id": final_source_id,
            "original_link": original_link,
            "description": leak_description,
            "leak_date": datetime.fromisoformat(leak_date),
            "severity": severity,
            "created_at": datetime.now(),
            "updated_at": datetime.now()
        }
        leak_result = db.leaks.insert_one(leak_doc)
        leak_id = leak_result.inserted_id

        index_name = f"leak_{str(leak_id)}"

        # 3. íŒŒì¼ ì²˜ë¦¬
        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.filename)[1]) as tmp_file:
            content = await file.read()
            tmp_file.write(content)
            tmp_file_path = tmp_file.name

        df = load_file_to_dataframe(tmp_file_path, file.content_type)

        # 4. Elasticsearch ì—…ë¡œë“œ
        success, failed = helpers.bulk(es, doc_generator(df, index_name))

        # 5. File ë©”íƒ€ë°ì´í„° ì €ì¥
        file_doc = {
            "leak_id": leak_id,
            "file_name": file_name,
            "index_name": index_name,
            "file_type": os.path.splitext(file.filename)[1],
            "uploaded_at": datetime.now()
        }
        db.files.insert_one(file_doc)

        os.unlink(tmp_file_path)

        return {
            "message": "ì—…ë¡œë“œ ì„±ê³µ",
            "source_id": str(final_source_id),
            "leak_id": str(leak_id),
            "index_name": index_name,
            "success": success,
            "failed": failed,
            "total_records": len(df)
        }

    except Exception as e:
        if 'tmp_file_path' in locals():
            os.unlink(tmp_file_path)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/")
async def root():
    """API ìƒíƒœ í™•ì¸"""
    return {"message": "DeepGuard Backend Running", "status": "running"}


@app.get("/health")
async def health_check():
    """Elasticsearch ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        return {"status": "ok", "elasticsearch": "connected" if es.ping() else "disconnected"}
    except Exception as e:
        return {"status": "error", "detail": str(e)}


@app.get("/indices")
async def get_indices():
    """ëª¨ë“  ì¸ë±ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        indices = es.indices.get_alias(index="*")
        index_list = [
            {"name": index_name, "aliases": list(info.get("aliases", {}).keys())}
            for index_name, info in indices.items() if not index_name.startswith(".")
        ]
        return {"total": len(index_list), "indices": index_list}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/fields/{index_name}")
async def get_fields(index_name: str):
    """ì§€ì •í•œ ì¸ë±ìŠ¤ì˜ í•„ë“œ(ì¹¼ëŸ¼) ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        if not es.indices.exists(index=index_name):
            raise HTTPException(status_code=404, detail=f"ì¸ë±ìŠ¤ '{index_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        mapping = es.indices.get_mapping(index=index_name)
        fields = []
        properties = mapping[index_name]['mappings'].get('properties', {})

        for field_name, field_info in properties.items():
            fields.append({
                "name": field_name,
                "type": field_info.get('type', 'object')
            })

        return {"index_name": index_name, "total_fields": len(fields), "fields": fields}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/data/{index_name}")
async def get_data(index_name: str, size: int = 100, from_: int = 0):
    """ì§€ì •í•œ ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    try:
        if not es.indices.exists(index=index_name):
            raise HTTPException(status_code=404, detail=f"ì¸ë±ìŠ¤ '{index_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        response = es.search(
            index=index_name,
            body={"query": {"match_all": {}}, "size": size, "from": from_}
        )
        hits = response['hits']['hits']
        documents = [hit['_source'] for hit in hits]

        return {
            "index_name": index_name,
            "total": response['hits']['total']['value'],
            "size": len(documents),
            "from": from_,
            "documents": documents
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/search/{index_name}")
async def search_data(index_name: str, query: str, field: str = None, size: int = 100, min_score: float = None):
    """ì§€ì •í•œ ì¸ë±ìŠ¤ì—ì„œ ê²€ìƒ‰"""
    try:
        if not es.indices.exists(index=index_name):
            raise HTTPException(status_code=404, detail=f"ì¸ë±ìŠ¤ '{index_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        search_query = {"query": {}, "size": size}
        if field:
            search_query["query"] = {"match": {field: query}}
        else:
            search_query["query"] = {"multi_match": {"query": query, "type": "best_fields"}}

        if min_score is not None:
            search_query["min_score"] = min_score

        response = es.search(index=index_name, body=search_query)
        hits = response['hits']['hits']
        documents = [hit['_source'] for hit in hits]
        scores = [hit['_score'] for hit in hits]

        return {
            "index_name": index_name,
            "query": query,
            "total": response['hits']['total']['value'],
            "size": len(documents),
            "documents": documents,
            "scores": scores
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/search-all-indices")
async def search_all_indices(email: str, size: int = 100):
    """ëª¨ë“  ì¸ë±ìŠ¤ì—ì„œ ì´ë©”ì¼ ê²€ìƒ‰"""
    try:
        all_indices = es.indices.get_alias(index="*")
        index_names = [name for name in all_indices.keys() if not name.startswith(".")]

        if not index_names:
            return {"email": email, "total_indices_searched": 0, "results": []}

        search_query = {"query": {"multi_match": {"query": email, "type": "phrase", "fields": ["*"]}}, "size": size}
        results = []
        total_found = 0

        for index_name in index_names:
            try:
                response = es.search(index=index_name, body=search_query)
                hits = response['hits']['hits']
                if hits:
                    documents = [{**hit['_source'], "_score": hit['_score']} for hit in hits]
                    results.append({
                        "index_name": index_name,
                        "total_hits": response['hits']['total']['value'],
                        "returned_hits": len(documents),
                        "documents": documents
                    })
                    total_found += response['hits']['total']['value']
            except:
                continue

        return {
            "email": email,
            "total_indices_searched": len(index_names),
            "total_documents_found": total_found,
            "indices_with_results": len(results),
            "results": results
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


def mask_text(text: str):
    """ë§ˆìŠ¤í‚¹"""
    if not text or len(text) <= 2: return text
    return text[0] + "*" * (len(text) - 2) + text[-1]


def get_sha256(text: str):
    """í•´ì‹±"""
    return hashlib.sha256(text.encode()).hexdigest()


@app.post("/parse-file")
async def parse_user_file(file: UploadFile = File(...)):
    """íŒŒì¼ íŒŒì‹± ë° ë§ˆìŠ¤í‚¹ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)"""
    content = await file.read()
    try:
        text_data = content.decode("utf-8")
    except:
        text_data = content.decode("cp949", errors="ignore")

    results = []
    lines = text_data.splitlines()

    for line in lines:
        line = line.strip()
        if not line: continue

        if ":" in line:
            parts = line.split(":")
            target_email = ""
            target_pw = ""

            for i, part in enumerate(parts):
                if "@" in part and "." in part:
                    target_email = part.strip()
                    if i + 1 < len(parts):
                        target_pw = parts[i + 1].strip()
                    break

            if target_email:
                try:
                    email_id, email_domain = target_email.split("@", 1)
                except:
                    email_id = target_email
                    email_domain = ""

                entry = {
                    "id": str(uuid.uuid4()),
                    "original_hash": get_sha256(target_email),
                    "masked_email": f"{mask_text(email_id)}@{email_domain}",
                    "masked_password": mask_text(target_pw),
                    "raw_text": line,
                    "status": "Ready for Crawling"
                }
                results.append(entry)

    return {"status": "success", "file_name": file.filename, "total_parsed": len(results), "data": results}


def check_ransomware_risk(domain_keyword: str):
    """OSINT ëœì„¬ì›¨ì–´ ì²´í¬ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)"""
    url = "https://api.ransomware.live/v2/recentvictims"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, verify=False, timeout=5)
        if response.status_code != 200:
            return {"status": "Error", "message": "API Error"}

        data = response.json()
        for item in data:
            victim = item.get('victim', '').lower()
            if domain_keyword.lower() in victim:
                return {
                    "is_leaked": True,
                    "victim_name": item.get('victim'),
                    "group": item.get('group'),
                    "date": item.get('attackdate'),
                    "screenshot": item.get('screenshot', ''),
                    "risk_level": "Critical"
                }
        return {"is_leaked": False, "risk_level": "Safe"}
    except Exception as e:
        return {"is_leaked": False, "error": str(e)}


# ---------------------------------------------------------
# [UPDATED] í¬ë¡¤ëŸ¬ê°€ ì—°ë™ëœ Analyze File (ê¸°ëŠ¥ ì—…ê·¸ë ˆì´ë“œ)
# ---------------------------------------------------------
@app.post("/analyze-file")
async def analyze_uploaded_file(file: UploadFile = File(...)):
    print(f"ğŸ“‚ [DEBUG] íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ ì‹œì‘: {file.filename}")

    # 1. íŒŒì¼ ì½ê¸°
    content = await file.read()
    try:
        text_data = content.decode("utf-8")
    except:
        text_data = content.decode("cp949", errors="ignore")

    final_results = []


    try:
        dga_results = dga.analyze_text(
            text=text_data,
            filename=file.filename,
            mask=True  # ë§ˆìŠ¤í‚¹ ì ìš©
        )

        if dga_results:
            for item in dga_results:
                # DGA ê²°ê³¼ ìŠ¤í‚¤ë§ˆ ë§¤í•‘
                formatted_dga = {
                    "id": item.get("id", str(uuid.uuid4())),
                    "keyword_type": item.get("keyword_type", "asset"),  # ë¶„ì„ê¸°ëŠ” ì£¼ë¡œ ìì‚°/í‚¤ ë°œê²¬

                    # [ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜] íŒŒì¼ ë¶„ì„ì´ë¯€ë¡œ Source IDëŠ” 'File Analysis' ë˜ëŠ” íŒŒì¼ëª…
                    "source_id": f"File: {file.filename}",

                    "original_link": file.filename,
                    "raw_text": item.get("raw_text", ""),
                    "leak_date": item.get("leak_date", str(datetime.now())),

                    # [ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜] ë¶„ì„ê¸°ê°€ ê³„ì‚°í•œ severity ì‚¬ìš©
                    "severity": item.get("severity", "High")
                }
                final_results.append(formatted_dga)
            print(f"âœ… [Analyzer] íŒŒì¼ ë‚´ë¶€ ë¶„ì„ ì™„ë£Œ: {len(dga_results)}ê±´ ë°œê²¬")
    except Exception as e:
        print(f"âš ï¸ [Analyzer] ë¶„ì„ ì¤‘ ì˜¤ë¥˜ (Skip): {e}")

    # -----------------------------------------------------
    # [STEP B] ê¸°ì¡´ í¬ë¡¤ëŸ¬ (Crawler Controller) ì‹¤í–‰
    # ëª©ì : íŒŒì¼ ë‚´ ì´ë©”ì¼ì„ ì¶”ì¶œí•˜ì—¬ ì™¸ë¶€(í…”ë ˆê·¸ë¨/ë‹¤í¬ì›¹) ê²€ìƒ‰
    # -----------------------------------------------------
    lines = text_data.splitlines()
    print(f"ğŸ”¢ [Crawler] í…ìŠ¤íŠ¸ ë¼ì¸ ìŠ¤ìº” ì‹œì‘ ({len(lines)}ì¤„)")

    for line in lines:
        line = line.strip()
        if not line: continue

        target_email = ""
        target_pw = ""  # (í•„ìš”ì‹œ ì‚¬ìš©)

        # ê°„ë‹¨ íŒŒì‹± ë¡œì§ (ì´ë©”ì¼ ì¶”ì¶œ)
        if ":" in line:
            parts = line.split(":")
            for idx, part in enumerate(parts):
                if "@" in part and "." in part:
                    target_email = part.strip()
                    if idx + 1 < len(parts):
                        target_pw = parts[idx + 1].strip()
                    break
        elif "@" in line and "." in line:
            target_email = line.strip()

        # ì´ë©”ì¼ ë°œê²¬ ì‹œ í¬ë¡¤ëŸ¬ í˜¸ì¶œ
        if target_email:
            try:
                clean_email = target_email.replace('"', '').replace(',', '').strip()
                # ë¹„ë°€ë²ˆí˜¸ëŠ” í•„ìš”ì‹œ clean_pw ë³€ìˆ˜ì— ì €ì¥í•˜ì—¬ ì „ë‹¬

                print(f"ğŸ“¡ [Crawler] ì™¸ë¶€ ê²€ìƒ‰ ìš”ì²­: {clean_email}")

                # (1) í¬ë¡¤ëŸ¬ í˜¸ì¶œ (ë¹„ë™ê¸°)
                crawled_data = await main_controller(clean_email)
                # ë§Œì•½ main_controllerê°€ ì¸ì 2ê°œë¥¼ ë°›ëŠ”ë‹¤ë©´ main_controller(clean_email, target_pw)ë¡œ ìˆ˜ì •

                # ë°ì´í„° ë¦¬ìŠ¤íŠ¸í™”
                data_to_process = []
                if isinstance(crawled_data, list):
                    data_to_process = crawled_data
                elif isinstance(crawled_data, dict):
                    data_to_process = [crawled_data]

                # (2) ìŠ¤í‚¤ë§ˆ ë§¤í•‘ ë° ë³‘í•©
                if data_to_process:
                    for item in data_to_process:
                        # ====================================================
                        # â˜… [í•µì‹¬] í”„ë¡ íŠ¸ì—”ë“œ í‘œì¤€ ìŠ¤í‚¤ë§ˆ ì ìš© (ì¤€ê¸°ë‹˜ ìš”ì²­)
                        # ====================================================
                        formatted_crawl = {
                            "id": item.get("id", str(uuid.uuid4())),

                            # ìœ„í˜‘ ìœ í˜• (ê¸°ë³¸ê°’ credential)
                            "keyword_type": item.get("keyword_type", "credential"),

                            # â˜… [Source ID ë³€ê²½] í”Œë«í¼ëª…ì´ ì•„ë‹ˆë¼ 'ê²€ìƒ‰í•œ í‚¤ì›Œë“œ(Input)' ì…ë ¥
                            "source_id": clean_email,

                            # ì›ë³¸ ë§í¬ (ì—†ìœ¼ë©´ íŒŒì¼ëª…ì´ë¼ë„ ë„£ìŒ)
                            "original_link": item.get("original_link", item.get("url", file.filename)),

                            # ì›ë³¸ í…ìŠ¤íŠ¸
                            "raw_text": item.get("raw_text", item.get("text", line)),

                            # ìœ ì¶œ ì‹œì 
                            "leak_date": str(item.get("leak_date", datetime.now())),

                            # â˜… [Severity] í¬ë¡¤ëŸ¬ê°€ ê³„ì‚°í•œ ê°’ ê·¸ëŒ€ë¡œ ì „ë‹¬ (ì—†ìœ¼ë©´ Critical)
                            "severity": item.get("severity", "Critical")
                        }
                        # ====================================================

                        final_results.append(formatted_crawl)

                    print(f"   â””â”€ âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(data_to_process)}ê±´")

            except Exception as e:
                print(f"   â””â”€ âŒ í¬ë¡¤ë§ ì—ëŸ¬: {clean_email} - {e}")

    # -----------------------------------------------------
    # [STEP C] í†µí•© ë°ì´í„° DB ì €ì¥ ë° ë°˜í™˜
    # -----------------------------------------------------
    saved_count = 0
    if final_results:
        for item in final_results:
            try:
                # Elasticsearch ì €ì¥
                es.index(index="leaked_data", body=item)

                # MongoDB ì €ì¥
                if 'mongo_collection' in globals():
                    mongo_collection.insert_one(item.copy())

                saved_count += 1
            except Exception as e:
                print(f"âš ï¸ DB ì €ì¥ ì‹¤íŒ¨ (Skip): {e}")

            # MongoDB _id ê°ì²´ ì œê±° (JSON ë°˜í™˜ ìœ„í•´)
            if "_id" in item: del item["_id"]

    print(f"ğŸ† [ìµœì¢…] ì´ {len(final_results)}ê±´ ì²˜ë¦¬ ì™„ë£Œ (DB ì €ì¥: {saved_count}ê±´)")

    return {
        "status": "success",
        "file_name": file.filename,
        "total_processed": len(final_results),
        "data": final_results
    }


@app.patch("/api/leaks/{leak_id}/status")
async def update_leak_status(leak_id: str, status: str):
    """ìœ ì¶œ ì •ë³´ ìƒíƒœ ì—…ë°ì´íŠ¸"""
    try:
        valid_statuses = ["new", "processing", "investigating", "resolved"]
        if status not in valid_statuses:
            raise HTTPException(status_code=400, detail=f"Invalid status")
        result = db.leaks.update_one(
            {"_id": ObjectId(leak_id)},
            {"$set": {"status": status, "updated_at": datetime.now()}}
        )
        if result.matched_count == 0:
            raise HTTPException(status_code=404, detail="Leak not found")
        return {"message": "Status updated successfully", "status": status}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/leaks")
async def get_leaks(
        severity: Optional[str] = None,
        source_id: Optional[str] = None,
        limit: int = 100,
        skip: int = 0
):
    """ìœ ì¶œ ì •ë³´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        query = {}
        if severity: query["severity"] = severity
        if source_id: query["source_id"] = ObjectId(source_id)

        leaks = list(db.leaks.find(query).sort("leak_date", -1).skip(skip).limit(limit))
        total = db.leaks.count_documents(query)

        for leak in leaks:
            if 'source_id' in leak:
                source = db.sources.find_one({"_id": leak['source_id']})
                leak['source'] = serialize_doc(source) if source else None

            files = list(db.files.find({"leak_id": leak['_id']}))
            leak['files'] = [serialize_doc(f) for f in files]

        return {
            "total": total,
            "limit": limit,
            "skip": skip,
            "leaks": [serialize_doc(l) for l in leaks]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ---------------------------------------------------------
# [ëˆ„ë½ëœ ê¸°ëŠ¥ ë³µêµ¬] ìƒì„¸ ì¡°íšŒ ë° ê°œì¸ì •ë³´ í†µí•© ê²€ìƒ‰
# ---------------------------------------------------------

@app.get("/api/leaks/by-index/{index_name}")
async def get_leak_by_index(index_name: str):
    """ì¸ë±ìŠ¤ ì´ë¦„ìœ¼ë¡œ ìœ ì¶œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    try:
        # íŒŒì¼ì—ì„œ í•´ë‹¹ index_nameì„ ê°€ì§„ íŒŒì¼ ì°¾ê¸°
        file = db.files.find_one({"index_name": index_name})
        if not file:
            raise HTTPException(status_code=404, detail=f"Index '{index_name}'ì— ëŒ€í•œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # í•´ë‹¹ íŒŒì¼ì˜ leak ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        leak = db.leaks.find_one({"_id": file['leak_id']})
        if not leak:
            raise HTTPException(status_code=404, detail="Leak not found")

        # source ì •ë³´ ì¡°ì¸
        if 'source_id' in leak:
            source = db.sources.find_one({"_id": leak['source_id']})
            leak['source'] = serialize_doc(source) if source else None

        # íŒŒì¼ ì •ë³´ ì¡°ì¸ ë° ì¹¼ëŸ¼ ì •ë³´ ì¶”ê°€
        files = list(db.files.find({"leak_id": leak['_id']}))
        for f in files:
            if 'index_name' in f:
                try:
                    if es.indices.exists(index=f['index_name']):
                        mapping = es.indices.get_mapping(index=f['index_name'])
                        properties = mapping[f['index_name']]['mappings'].get('properties', {})
                        f['columns'] = [
                            {
                                'name': field_name,
                                'type': field_info.get('type', 'object')
                            }
                            for field_name, field_info in properties.items()
                        ]

                        count_response = es.count(index=f['index_name'])
                        f['record_count'] = count_response['count']
                except Exception as e:
                    f['columns'] = []
                    f['record_count'] = 0
        leak['files'] = [serialize_doc(f) for f in files]

        return serialize_doc(leak)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/search/personal-info")
async def search_personal_info(query: str, project_keyword: str = None, size: int = 100):
    """
    ê°œì¸ì •ë³´ ìœ ì¶œ ê²€ìƒ‰ (ì´ë©”ì¼ + í”„ë¡œì íŠ¸ í‚¤ì›Œë“œ)
    - [ìˆ˜ì •] ê²€ìƒ‰ ì‹œì‘ ì „, í•´ë‹¹ ì¿¼ë¦¬(ì´ë©”ì¼)ì— ëŒ€í•œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì—¬ ì¤‘ë³µ ë°©ì§€
    """
    try:
        print(f"ğŸ” [Search] ê²€ìƒ‰ ìš”ì²­: ì´ë©”ì¼={query}, í”„ë¡œì íŠ¸í‚¤ì›Œë“œ={project_keyword}")

        # -----------------------------------------------------
        # [STEP 0] ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¤‘ë³µ ëˆ„ì  ë°©ì§€)
        # -----------------------------------------------------
        try:
            # Elasticsearchì—ì„œ í•´ë‹¹ ì´ë©”ì¼ë¡œ ëœ ì´ì „ ê¸°ë¡ ì‚­ì œ
            es.delete_by_query(
                index="leaked_data",
                body={
                    "query": {
                        "match": {"target_email": query}
                    }
                },
                refresh=True  # ì‚­ì œ ì¦‰ì‹œ ë°˜ì˜
            )
            print(f"ğŸ§¹ [Clean] '{query}'ì— ëŒ€í•œ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘ ê²½ë¯¸í•œ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}")

        # -----------------------------------------------------
        # [STEP 1] ì‹¤ì‹œê°„ í¬ë¡¤ëŸ¬ ì‘ë™
        # -----------------------------------------------------
        try:
            crawled_data = await main_controller(query, project_keyword)

            data_to_process = []
            if isinstance(crawled_data, list):
                data_to_process = crawled_data
            elif isinstance(crawled_data, dict):
                data_to_process = [crawled_data]

            print(f"ğŸ•·ï¸ [Crawler] í¬ë¡¤ë§ ì™„ë£Œ: {len(data_to_process)}ê±´ ë°œê²¬")

            if data_to_process:
                for item in data_to_process:
                    formatted_item = {
                        "id": item.get("id", str(uuid.uuid4())),
                        # [ìˆ˜ì •] í¬ë¡¤ëŸ¬ê°€ ì¤€ keyword_typeì„ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ credential
                        "keyword_type": item.get("keyword_type", "credential"),
                        "source_id": query,
                        "source": item.get("source_id", "Unknown"),
                        "original_link": item.get("original_link", ""),
                        "raw_text": item.get("raw_text", ""),
                        "leak_date": str(item.get("leak_date", datetime.now())),
                        "target_email": item.get("target_email", query),
                        "found_keyword": item.get("found_keyword", query),
                        "severity": "Critical",
                        "status": "new",
                        "created_at": datetime.now()
                    }

                    es.index(index="leaked_data", body=formatted_item)

                    if 'mongo_collection' in globals():
                        mongo_collection.insert_one(formatted_item.copy())

                import time
                time.sleep(1)

        except TypeError as te:
            print(f"âš ï¸ í¬ë¡¤ëŸ¬ í˜¸ì¶œ íŒŒë¼ë¯¸í„° ì˜¤ë¥˜: {te}")
        except Exception as e:
            print(f"âš ï¸ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

        # -----------------------------------------------------
        # [STEP 2] ì €ì¥ëœ ë°ì´í„° ê²€ìƒ‰ (Elasticsearch)
        # -----------------------------------------------------
        all_indices = es.indices.get_alias(index="*")
        index_names = [name for name in all_indices.keys() if not name.startswith(".")]

        should_conditions = [
            {"match_phrase": {"target_email": query}},
            {"match_phrase": {"raw_text": query}}
        ]

        if project_keyword:
            should_conditions.append({"match_phrase": {"raw_text": project_keyword}})

        search_query = {
            "query": {
                "bool": {
                    "should": should_conditions,
                    "minimum_should_match": 1
                }
            },
            "size": size,
            "sort": [{"_score": "desc"}, {"created_at": "desc"}]
        }

        es_results = []

        for index_name in index_names:
            try:
                response = es.search(index=index_name, body=search_query)
                hits = response['hits']['hits']

                if hits:
                    for hit in hits:
                        es_results.append({
                            "index": index_name,
                            "score": hit['_score'],
                            "data": hit['_source']
                        })
            except:
                continue

        return {
            "query": query,
            "elasticsearch_results": es_results,
            "total_es_results": len(es_results)
        }

    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ API ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    # try:
    #     # Elasticsearchì—ì„œ ê²€ìƒ‰
    #     all_indices = es.indices.get_alias(index="*")
    #     index_names = [name for name in all_indices.keys() if not name.startswith(".")]
    #
    #     search_query = {
    #         "query": {
    #             "multi_match": {
    #                 "query": query,
    #                 "type": "best_fields",
    #                 "fields": ["*"]
    #             }
    #         },
    #         "min_score": 0.8,
    #         "size": size
    #     }
    #
    #     es_results = []
    #     index_info_map = {}
    #
    #     for index_name in index_names:
    #         try:
    #             response = es.search(index=index_name, body=search_query)
    #             hits = response['hits']['hits']
    #
    #             if hits:
    #                 # ì¸ë±ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (í•œ ë²ˆë§Œ)
    #                 if index_name not in index_info_map:
    #                     try:
    #                         # MongoDBì—ì„œ íŒŒì¼ ì •ë³´ ì°¾ê¸°
    #                         file_doc = db.files.find_one({"index_name": index_name})
    #
    #                         mapping = es.indices.get_mapping(index=index_name)
    #                         properties = mapping[index_name]['mappings'].get('properties', {})
    #                         columns = [
    #                             {
    #                                 'name': field_name,
    #                                 'type': field_info.get('type', 'object')
    #                             }
    #                             for field_name, field_info in properties.items()
    #                         ]
    #
    #                         # ì¸ë±ìŠ¤ì˜ ë¬¸ì„œ ê°œìˆ˜
    #                         count_response = es.count(index=index_name)
    #
    #                         index_info_map[index_name] = {
    #                             'index_name': index_name,
    #                             'file_name': file_doc['file_name'] if file_doc else index_name,
    #                             'columns': columns,
    #                             'total_records': count_response['count']
    #                         }
    #                     except:
    #                         index_info_map[index_name] = {
    #                             'index_name': index_name,
    #                             'file_name': index_name,
    #                             'columns': [],
    #                             'total_records': 0
    #                         }
    #
    #                 for hit in hits:
    #                     # scoreê°€ 0.8 ì´ìƒì¸ ê²°ê³¼ë§Œ ì¶”ê°€
    #                     if hit['_score'] >= 0.8:
    #                         es_results.append({
    #                             "index": index_name,
    #                             "score": hit['_score'],
    #                             "data": hit['_source'],
    #                             "index_info": index_info_map[index_name]
    #                         })
    #         except:
    #             continue
    #
    #     return {
    #         "query": query,
    #         "elasticsearch_results": es_results,
    #         "total_es_results": len(es_results),
    #         "indices_info": list(index_info_map.values())
    #     }
    # except Exception as e:
    #     raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn

    # 0.0.0.0ì€ ì™¸ë¶€ ì ‘ì† í—ˆìš©, portëŠ” 8000ë²ˆ í¬íŠ¸ ì‚¬ìš©
    uvicorn.run(app, host="0.0.0.0", port=8000)